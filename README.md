# Codveda Internship Tasks

This repository contains my internship work at **Codveda**, focusing on
**data collection**,  **data cleaning** to
**exploratory data analysis (EDA)**.
The project showcases practical skills in **web scraping, data
preprocessing, and data analysis** using Python.

------------------------------------------------------------------------

# Overview of Tasks

## **Task 1: Data Collection & Web Scraping**

**Goal:** Extract structured data from a target website.

### **What I Did**

-   Identified a target website and inspected its HTML structure.
-   Used **BeautifulSoup** and **requests** to scrape useful
    information.
-   Handled:
    -   Pagination
    -   Missing tags
    -   Basic anti-scraping measures
-   Saved the extracted data into **CSV/JSON** using `pandas`.

### **Tools & Libraries**

-   Python
-   BeautifulSoup4
-   requests
-   pandas

------------------------------------------------------------------------

## **Task 2: Data Cleaning & Preprocessing**

**Goal:** Convert unprocessed data into clean, analysis-ready data.

### **Cleaning Steps**

-   Handled missing values (removal & imputation).
-   Removed outliers.
-   Converted categorical variables using:
    -   Label encoding
-   Normalized or standardized numerical features.

### **Tools & Libraries**

-   pandas
-   numpy
-   scikit-learn

------------------------------------------------------------------------

## **Task 3: Exploratory Data Analysis (EDA)**

**Goal:** Understand patterns, distributions, and relationships in the
data.

### **Steps Performed**

-   Computed summary statistics (mean, median, mode, variance).
-   Plotted:
    -   Histograms
    -   Scatter plots
    -   Box plots
    -   Bar chars
-   Generated correlation matrices & heatmaps.
-   Identified trends, feature relationships, and initial insights.
-   Compiled findings into a concise EDA report.

### **Tools & Libraries**

-   pandas
-   matplotlib
-   seaborn
-   Jupyter Notebook

------------------------------------------------------------------------

# Technologies Used

-   Python 3.x
-   BeautifulSoup & requests
-   pandas, numpy
-   scikit-learn
-   matplotlib, seaborn
-   Jupyter Notebook

------------------------------------------------------------------------

# Learning Outcomes

Through this tasks, I gained hands-on experience in:

-   Web scraping real-world websites
-   Cleaning messy, unstructured datasets
-   Handling missing data & outliers
-   Encoding categorical variables
-   Normalizing numerical data
-   Producing professional EDA reports
-   Using Python for full data workflows

------------------------------------------------------------------------

# Acknowledgements

Special thanks to **Codveda** for providing guidance and resources
during my internship and enabling me to work on industry-relevant data
tasks.
